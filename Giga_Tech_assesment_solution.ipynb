{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Data Analysis and Preprocessing**\n",
        "\n",
        "Load and Clean Data\n"
      ],
      "metadata": {
        "id": "-YQca0L28E0W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOLzrlfY7-_f",
        "outputId": "7a089ce1-d07a-4f19-892d-13787f9dd8b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        token pos_tag ner_tag\n",
            "1      শনিবার     NNP   B-D&T\n",
            "2         (২৭   PUNCT   B-OTH\n",
            "3      আগস্ট)     NNP   B-D&T\n",
            "4        রাতে     NNC   B-D&T\n",
            "5  পটুয়াখালী     NNP   B-GPE\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/data.tsv', sep='\\t', names=['token', 'pos_tag', 'ner_tag'])\n",
        "\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization and Padding**\n",
        "Tokenize and pad sequences using TensorFlow and Keras utilities."
      ],
      "metadata": {
        "id": "Q83B1q_t8gYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/data.tsv', sep='\\t', names=['token', 'pos_tag', 'ner_tag'])\n",
        "\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['token'])\n",
        "sequences = tokenizer.texts_to_sequences(data['token'])\n",
        "padded_sequences = pad_sequences(sequences, padding='post')\n"
      ],
      "metadata": {
        "id": "C6sO9xgGAYxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Label Encoding and One-Hot Encoding**\n",
        "This section is about encoding categorical labels for POS and NER tagging into a numeric format that can be processed by the neural network, including one-hot encoding to transform these labels into a binary class matrix."
      ],
      "metadata": {
        "id": "lYxhM7gH8wo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Label encoding\n",
        "label_encoder_pos = LabelEncoder()\n",
        "label_encoder_ner = LabelEncoder()\n",
        "\n",
        "data['pos_encoded'] = label_encoder_pos.fit_transform(data['pos_tag'])\n",
        "data['ner_encoded'] = label_encoder_ner.fit_transform(data['ner_tag'])\n",
        "\n",
        "# One-hot encoding\n",
        "pos_onehot = to_categorical(data['pos_encoded'], num_classes=len(label_encoder_pos.classes_))\n",
        "ner_onehot = to_categorical(data['ner_encoded'], num_classes=len(label_encoder_ner.classes_))\n"
      ],
      "metadata": {
        "id": "qKbIMxCfA4Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Padding Labels**\n",
        "To ensure the labels match the length of input sequences, this part includes padding the one-hot encoded labels to create uniform input for training."
      ],
      "metadata": {
        "id": "1F-xmA4U81GQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def pad_labels_to_match_sequences(onehot_labels, seq_length):\n",
        "    padded_labels = np.zeros((len(onehot_labels), seq_length, onehot_labels.shape[1]))\n",
        "    for i, seq in enumerate(onehot_labels):\n",
        "        padded_labels[i, :len(seq), :] = seq\n",
        "    return padded_labels\n",
        "\n",
        "pos_labels_padded = pad_labels_to_match_sequences(pos_onehot, padded_sequences.shape[1])\n",
        "ner_labels_padded = pad_labels_to_match_sequences(ner_onehot, padded_sequences.shape[1])\n",
        "combined_labels = np.concatenate([pos_labels_padded, ner_labels_padded], axis=-1)\n"
      ],
      "metadata": {
        "id": "XNkxW4Tp84Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Model Definition and Compilation**\n",
        "This section involves setting up the neural network architecture, including embedding layers, LSTM layers, and the output layer, followed by compiling the model with appropriate loss function and metrics."
      ],
      "metadata": {
        "id": "ubIiYxdlBDGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(combined_labels.shape[2], activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "4uQA6-29BZbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Model Training and Evaluation**\n",
        "This final step involves training the model on the training data and evaluating its performance on the test set. This helps understand the model’s accuracy and other performance metrics."
      ],
      "metadata": {
        "id": "xCLcqy_cBeXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, combined_labels, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEQO8z8oBhvn",
        "outputId": "5c3be473-b6f2-45b1-aed5-40583983f5c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m992/992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 64ms/step - accuracy: 0.1018 - loss: 10.7672 - val_accuracy: 0.1009 - val_loss: 42.3872\n",
            "Epoch 2/5\n",
            "\u001b[1m992/992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 56ms/step - accuracy: 0.1124 - loss: 61.9704 - val_accuracy: 0.1010 - val_loss: 107.3848\n",
            "Epoch 3/5\n",
            "\u001b[1m992/992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 56ms/step - accuracy: 0.1171 - loss: 128.8853 - val_accuracy: 0.1010 - val_loss: 174.9969\n",
            "Epoch 4/5\n",
            "\u001b[1m992/992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 57ms/step - accuracy: 0.1184 - loss: 197.2652 - val_accuracy: 0.1010 - val_loss: 252.4154\n",
            "Epoch 5/5\n",
            "\u001b[1m992/992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 57ms/step - accuracy: 0.1199 - loss: 278.5403 - val_accuracy: 0.2269 - val_loss: 330.6854\n",
            "\u001b[1m284/284\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.2254 - loss: 312.7773\n",
            "Test Accuracy: 0.2240431010723114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Performance Metrics:**"
      ],
      "metadata": {
        "id": "ZZc8LItutRur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "y_pred_flat = []\n",
        "y_true_flat = []\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    length = np.sum(X_test[i] != 0)\n",
        "    y_pred_flat.extend(y_pred_classes[i][:length])\n",
        "    y_true_flat.extend(y_test[i][:length])\n",
        "\n",
        "y_pred_flat = np.array(y_pred_flat)\n",
        "y_true_flat = np.array(y_true_flat)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_flat, y_pred_flat, zero_division=0))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNh3e4PECE3N",
        "outputId": "89114dab-4711-4b9f-e4b4-29b5f15e47e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m284/284\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00    8182.0\n",
            "         1.0       0.00      0.00      0.00     948.0\n",
            "         5.0       0.00      0.00      0.00       0.0\n",
            "        22.0       0.00      0.00      0.00       0.0\n",
            "\n",
            "    accuracy                           0.00    9130.0\n",
            "   macro avg       0.00      0.00      0.00    9130.0\n",
            "weighted avg       0.00      0.00      0.00    9130.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.save('ner_pos_model.h5')\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "deployed_model = load_model('ner_pos_model.h5')\n",
        "\n",
        "def make_prediction(text):\n",
        "    tokenized_sequence = tokenizer.texts_to_sequences([text])\n",
        "    padded_sequence = pad_sequences(tokenized_sequence, maxlen=padded_sequences.shape[1], padding='post')\n",
        "    prediction = deployed_model.predict(padded_sequence)\n",
        "    predicted_tags = np.argmax(prediction, axis=-1)[0]\n",
        "    return predicted_tags\n",
        "\n",
        "# Example usage\n",
        "text_example = \"example text for testing\"\n",
        "print(make_prediction(text_example))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KULe7FKZqr2",
        "outputId": "219ac036-a556-4fc4-c859-a0df8e08d409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
            "[ 5  5  5  5  5  5  5  5  5 22 22 22  5  5  5  5 22 22 22 22  9  9 22 22\n",
            " 22 10 18 18 18 18 18 18 18 18 18 18 18 18 18 18  9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Et_Y6hPaaH4W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}